{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvAwvJEPDdrNbUsPM95viG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lencons/ai-image-processing/blob/main/text-to-image/stable_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook does something....."
      ],
      "metadata": {
        "id": "S9W6zdOQBp2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Initialisation"
      ],
      "metadata": {
        "id": "ph-azTZGEuDd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "407XDfQwASHt"
      },
      "outputs": [],
      "source": [
        "# Install all required Python libraries\n",
        "!pip install transformers diffusers lpips "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to authenticate with HuggingFace so we can download the stable-defusion model\n",
        "#     the authentication token can be found under Profile->Access Tokens\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "SofMuRREBkic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
        "from tqdm.auto import tqdm\n",
        "from torch import autocast\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy\n",
        "from torchvision import transforms as tfms\n",
        "\n",
        "# For video display:\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "# Set processing device\n",
        "#\n",
        "#  CPU  - 50min per image\n",
        "#  CUDA -\n",
        "#  TPU  -\n",
        "#\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f'Processing Device: {torch_device}')"
      ],
      "metadata": {
        "id": "VmwCQb1oCpHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the autoencoder model which will be used to decode the latents into image space. \n",
        "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_auth_token=True)\n",
        "\n",
        "# Load the tokenizer and text encoder to tokenize and encode the text. \n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# The UNet model for generating the latents.\n",
        "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_auth_token=True)\n",
        "\n",
        "# The noise scheduler\n",
        "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "\n",
        "# To the GPU we go!\n",
        "vae = vae.to(torch_device)\n",
        "text_encoder = text_encoder.to(torch_device)\n",
        "unet = unet.to(torch_device)"
      ],
      "metadata": {
        "id": "T7rFa98UDM6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single Image Generation"
      ],
      "metadata": {
        "id": "mE0KbJ2IFDtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\"rain forest hiding a waterfall, vines hanging, misty, 4k, detailed\"]\n",
        "height = 512\n",
        "width = 768\n",
        "num_inference_steps = 50\n",
        "guidance_scale = 7.5\n",
        "generator = torch.manual_seed(120)\n",
        "batch_size = 1\n",
        "\n",
        "# Prep text \n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "# Prep Scheduler\n",
        "scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "# Prep latents\n",
        "latents = torch.randn(\n",
        "(batch_size, unet.in_channels, height // 8, width // 8),\n",
        "generator=generator,\n",
        ")\n",
        "\n",
        "latents = latents.to(torch_device)\n",
        "latents = latents * scheduler.sigmas[0] # Need to scale to match k\n",
        "\n",
        "# Loop\n",
        "with autocast(\"cuda\"):\n",
        "    for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "        latent_model_input = torch.cat([latents] * 2)\n",
        "        sigma = scheduler.sigmas[i]\n",
        "        latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "        # predict the noise residual\n",
        "        with torch.no_grad():\n",
        "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "        # perform guidance\n",
        "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "        # compute the previous noisy sample x_t -> x_t-1\n",
        "        latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]\n",
        "\n",
        "# scale and decode the image latents with vae\n",
        "latents = 1 / 0.18215 * latents\n",
        "\n",
        "with torch.no_grad():\n",
        "    image = vae.decode(latents).sample\n",
        "\n",
        "# Display\n",
        "image = (image / 2 + 0.5).clamp(0, 1)\n",
        "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "images = (image * 255).round().astype(\"uint8\")\n",
        "pil_images = [Image.fromarray(image) for image in images]\n",
        "pil_images[0]"
      ],
      "metadata": {
        "id": "yWxlGpH9Dc-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Seed Loop"
      ],
      "metadata": {
        "id": "vfpWKgJYN0Am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output is written to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Gjk08zaxN7qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a folder to store results\n",
        "!rm -rf /content/drive/MyDrive/stablediff/futuristic\n",
        "!mkdir -p /content/drive/MyDrive/stablediff/futuristic\n",
        "\n",
        "# Some settings\n",
        "prompt = [\"a futuristic city, abandoned and overgrown with plants, dystopia, bathed in sunlight\"]\n",
        "#prompt = [\"an electric sky city on an alien world\"]\n",
        "height = 512                        # default height of Stable Diffusion\n",
        "width = 768                         # default width of Stable Diffusion\n",
        "num_inference_steps = 50            # Number of denoising steps\n",
        "guidance_scale = 8.0                # Scale for classifier-free guidance\n",
        "generator = torch.manual_seed(1200)   # Seed generator to create the inital latent noise\n",
        "batch_size = 1\n",
        "\n",
        "# Prep text \n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "# Prep Scheduler\n",
        "scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "for idx in range(100):\n",
        "    generator = torch.manual_seed(idx)\n",
        "\n",
        "    # Prep latents\n",
        "    latents = torch.randn(\n",
        "    (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        "    )\n",
        "    latents = latents.to(torch_device)\n",
        "    latents = latents * scheduler.sigmas[0] # Need to scale to match k\n",
        "\n",
        "    # Loop\n",
        "    with autocast(\"cuda\"):\n",
        "        for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "            # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "            latent_model_input = torch.cat([latents] * 2)\n",
        "            sigma = scheduler.sigmas[i]\n",
        "            latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "            # predict the noise residual\n",
        "            with torch.no_grad():\n",
        "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "            # perform guidance\n",
        "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]\n",
        "\n",
        "    # scale and decode the image latents with vae\n",
        "    latents = 1 / 0.18215 * latents\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image = vae.decode(latents).sample\n",
        "    \n",
        "    # Display\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "    images = (image * 255).round().astype(\"uint8\")\n",
        "    pil_images = [Image.fromarray(image) for image in images]\n",
        "    pil_images[0].save(f'/content/drive/MyDrive/stablediff/futuristic/{idx:04}.jpeg')"
      ],
      "metadata": {
        "id": "QjkWvHsnN47n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}